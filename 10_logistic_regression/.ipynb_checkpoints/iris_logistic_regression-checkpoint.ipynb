{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf555cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272279aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris(as_frame=True)\n",
    "df = data.frame\n",
    "df = df[df.iloc[:, -1] != 2]\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee495a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of some features (100, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]]\n",
      "(100, 4) \n",
      "\n",
      "Labels (100,)\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example of some features {X.shape}\\n{X[:8]}\")\n",
    "print(X.shape, \"\\n\")\n",
    "print(f\"Labels {y.shape}\\n {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee116df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of bundle:\n",
      "Train: X -> (80, 4) y -> (80,)\n",
      "Test:  X -> (20, 4) y -> (20,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=666)\n",
    "\n",
    "print(f\"Size of bundle:\\nTrain: X -> {X_train.shape} y -> {y_train.shape}\\nTest:  X -> {X_test.shape} y -> {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2012daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES  = X_train.shape[1]\n",
    "WEIGHT_VECTOR_SIZE = num_features\n",
    "WEIGHT_VECTOR = np.zeros(WEIGHT_VECTOR_SIZE)\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.05\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5182c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid_activation(z):\n",
    "    return (1 / (1 + np.exp(-z)))    \n",
    "\n",
    "# Linear calculation:\n",
    "# Algo -> z = X . W + b\n",
    "def linear_calc(X, W, b):\n",
    "    return sigmoid_activation(np.dot(X, W) + b)\n",
    "\n",
    "# Classify based on given probability\n",
    "def probability_norm(prob):\n",
    "    if (prob >= .5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Binary cross entropy loss function\n",
    "def loss(y, y_hat):\n",
    "    binary_cross_entropy = -np.mean((y * np.log(y_hat)) + (1 - y) * (np.log(1 - y_hat)))\n",
    "    return binary_cross_entropy\n",
    "    \n",
    "# Compute gradient using:\n",
    "#     dLoss/dW = (1/m) X^T (y_hat - y)\n",
    "#     dLoss/db = (1/m) SUM (y_hat - y)\n",
    "def compute_gradient(X, y, y_hat, delta, NUM_SAMPLES):\n",
    "    dW = (1 / NUM_SAMPLES) * np.dot(X.T, delta)\n",
    "    db = (1 / NUM_SAMPLES) * np.sum(delta)\n",
    "    return dW, db\n",
    "\n",
    "# Update parameters using:\n",
    "#     W = W - η*(dLoss/dW)\n",
    "#     b = b - η*(dLoss/db)\n",
    "# where η is the learning rate\n",
    "def grad_descent(learning_rate, W, dW, b, db):\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ee1cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "# Forward pass -> run linear calc + sigmoid\n",
    "# Compute gradients w.r.t. W and b\n",
    "# Update W and b\n",
    "# Calculate loss using binary-cross-entropy\n",
    "def train(X, y, W, b, num_samples, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = linear_calc(X, W, b)\n",
    "        delta = y_hat - y\n",
    "        dW, db = compute_gradient(X, y, y_hat, delta, num_samples)\n",
    "        W, b = W, b\n",
    "        W, b = grad_descent(learning_rate, W, dW, b, db)\n",
    "        bce = loss(y, y_hat)\n",
    "        print(f\"Epoch {epoch}, Loss {bce}\")\n",
    "    return W, b\n",
    "\n",
    "# Test function:\n",
    "# Foward pass -> run linear calc again\n",
    "# Predict     -> Sigmoid function + >=0.5 is 1 otherwise 0\n",
    "# Accuracy    -> where y_hat == y\n",
    "def test(X, y, W, b):\n",
    "    y_hat = linear_calc(X, W, b)\n",
    "    print(f\"Raw predictions:\\n{y_hat}\\n\")\n",
    "    prediction = np.zeros(len(y_hat))\n",
    "    \n",
    "    for i in range(len(y_hat)):\n",
    "        prediction[i] = probability_norm(y_hat[i])\n",
    "    \n",
    "    print(f\"Real labels:\\n{y}\\n\")\n",
    "    print(f\"Predicted labels:\\n{prediction.astype(int)}\\n\")\n",
    "    \n",
    "    accuracy = np.mean(prediction == y) * 100\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bd433e14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.08006641523600662\n",
      "Epoch 1, Loss 0.07963348992674682\n",
      "Epoch 2, Loss 0.07920537276462819\n",
      "Epoch 3, Loss 0.07878195912194548\n",
      "Epoch 4, Loss 0.07836315476257882\n",
      "Epoch 5, Loss 0.07794887307432223\n",
      "Epoch 6, Loss 0.0775390331565949\n",
      "Epoch 7, Loss 0.07713355849350853\n",
      "Epoch 8, Loss 0.07673237602830621\n",
      "Epoch 9, Loss 0.07633541551360337\n"
     ]
    }
   ],
   "source": [
    "num_samples = X_train.shape[0]\n",
    "W, b = train(X_train, y_train, WEIGHT_VECTOR, bias, num_samples, LEARNING_RATE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d9bf7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions:\n",
      "[0.96079601 0.95692111 0.06379348 0.05566304 0.10665098 0.08205874\n",
      " 0.06585298 0.98500068 0.95899577 0.96249864 0.05462559 0.03644135\n",
      " 0.10459533 0.0585129  0.9576279  0.93585425 0.86331649 0.13373765\n",
      " 0.0651067  0.10110097]\n",
      "\n",
      "Real labels:\n",
      "[1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0]\n",
      "\n",
      "Predicted labels:\n",
      "[1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0]\n",
      "\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "test(X_test, y_test, W, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
